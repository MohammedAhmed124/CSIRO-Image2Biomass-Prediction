{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86bcecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammed/miniconda3/envs/bioenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/mohammed/miniconda3/envs/bioenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/mohammed/miniconda3/envs/bioenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler, autocast # NEW: Use robust torch.amp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import shutil\n",
    "import random\n",
    "from typing import Callable, Optional, Dict, Any, Tuple, List\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c274aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import CFG\n",
    "def get_id(path):\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "base_directory = CFG.DATA_DIR\n",
    "train_csv = pd.read_csv(\n",
    "    os.path.join(base_directory , \"train.csv\")\n",
    ")\n",
    "train_imgs_path = glob.glob(\\\n",
    "    os.path.join(base_directory , \"train\" , \"*.jpg\")\n",
    ")\n",
    "\n",
    "dict_ = defaultdict(list)\n",
    "\n",
    "for path in train_imgs_path:\n",
    "    id_ = get_id(path)\n",
    "    cols_to_include = [\"target_name\" , \"target\" , \"Sampling_Date\" , \"State\" ,\"Pre_GSHH_NDVI\" ]\n",
    "    img_info = train_csv[train_csv[\"sample_id\"].str.split(\"_\" , expand=True)[0]==id_][cols_to_include].copy()\n",
    "    extra_info = img_info[[ \"Sampling_Date\" , \"State\" ,\"Pre_GSHH_NDVI\"]].iloc[0,:].to_dict()\n",
    "    info_dict = img_info.set_index(\"target_name\").to_dict()['target']\n",
    "    info_dict[\"id\"] = id_\n",
    "    info_dict[\"image_path\"] = path\n",
    "    info_dict.update(extra_info)\n",
    "    for k  , v in info_dict.items():\n",
    "        dict_[k].append(v)\n",
    "\n",
    "df = pd.DataFrame(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5f47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  7,  11,  13,  14,  16,  17,  33,  36,  48,  50,  55,  58,  76,  77,\n",
       "        78,  79,  80,  81,  85,  86,  94,  95,  96,  97, 105, 114, 122, 123,\n",
       "       129, 130, 131, 140, 150, 166, 170, 174, 178, 181, 189, 190, 201, 203,\n",
       "       211, 212, 217, 218, 219, 228, 233, 235, 237, 240, 248, 254, 264, 286,\n",
       "       302, 303, 315, 319, 320, 323, 326, 330, 343, 344, 345, 347, 349, 350,\n",
       "       353, 354],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.Kfold import create_folds\n",
    "\n",
    "s = create_folds(\n",
    "    df , strategy=\"stratified\",random_state=42, stratify_col=\"State\" , n_splits=5\n",
    ")\n",
    "df[\"fold\"] = s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc1738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
